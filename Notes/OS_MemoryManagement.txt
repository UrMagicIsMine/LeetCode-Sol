----------------------        Ch 8 Main Memory          ------------------------

Background
1) registers are generally accessible within one cycle of the CPU clock; Completing
a memory access take many cycles of CPU clock. (stall)
2) protect user space from one another: base register + limit register (load by
special privileged instruction). illegal attempts results in a trap to system as
fatal error;
3) Compiler bins symbolic address to relocatable addresses. The binding of
instructions and data to memory addresses compile time/load time/run time;
3) Logical address (generated by CPU) vs physical address (seen by memory unit)
4) Mapping from virtual to physical is down by hardware device (MMU), The user
program never sees the real physical addresses;
5) Dynamic loading, loaded when called. (users responsibility)
6) Dynamic linked libraries, linked to user programs until execution.(e.g. system
language libraries, would waste of both disk/main memory space). Need OS support

Swapping
1) A process can be swapped temporarily out of memory to a backing store and then
brought back into memory for continued execution; time Proportional to memory amount
2) swapping constrain: 1) process with pending IO -> double buffer

Contiguous Memory Allocation
1) The memory is usually divided into two partitions: for resident operating system
and for the user processes.
2) Memory allocation with variable-partition scheme: one large block -> a set of
holes of various size scattered throughout memory. first-fit/best-fit/worst-fit
3) external fragmentation: enough total memory space to satisfy a request but the
available spaces are not contiguous; internal fragmentation: unused memory that
is internal to a partition;
4) Compaction: move process to one end, holes to other end (expensive)

Segmentation
1) Segmentation is a memory-management scheme that supports this programmer
view of memory. (code/global variable/heap/stack/standard C lib)
2) A logical address consists of a two tuple: <segment-number, offset>.
3) Mapping is effected by a segment table (segment base and segment limit)

Paging
1) Basic method involves breaking physical memory into fixed-sized blocks (frames)
and breaking logical memory into blocks of same size called pages;
2) Address generated by CPU is divided into 1) page number and 2) page offset.
Page number is index in page table contains base address of page in physical memory
3) The selection of a power of 2 as a page size makes the translation of a
logical address into a page number and page offset particularly easy. High-order
bits designate page number and low-order bits designate page offset.
4) Use paging, we have no external but still have some internal fragmentation
5ï¼‰The operating system manage the frame using frame table(one entry for each
physical page frame). It also maintains a copy of the page table for each process
to translate logical addresses to physical addresses.
5) Page table is kept in main memory and a page-table base register (PTBR) points
to the page table, reducing context-switch time; (2 memory access)
6) a translation look-aside buffer (TLB) is a special, small and fast lookup
hardware cache to speedup memory access; (page number/frame number pair)
7) TLB miss/full and selection of replacement/wired down for kernel code
8) Some TLBs store address-space identifiers in each TLB entry to provide
protection for that process;
9) hit ratio/effective memory-access time
10) protection bits associated with each frame in the page table can define a
page to be read-write or read-only, valid or invalid;
11) Another advantage of paging is the possibility of sharing common code (reentrant
code), some OS implement shared memory using shared pages;

Structure of the Page Table
1) Hierarchical Paging (inappropriate for 64 bit architecture)
2) Hashed Page tables with hash value being the virtual page number
3) Inverted Page Tables: <process-id, page-number, offset>.
  - sorted by physical address, lookups occurs on virtual addresses
  - difficulty implementing shared Memory


----------------------       Ch 9 Virtual Memory        ------------------------

Background
1) Virtual memory involves the separation of logical memory as perceived
by users from physical memory;
  - allows an extremely large virtual memory to be provided for programmer when
    only a smaller physical memory is available
2) The large blank space between the heap and the stack is part of the virtual
address space but will require actual physical pages only if the heap or stack
grows;
3) Virtual memory allows files and memory to be shared among processes through
PAGE SHARING
  - System libraries can be shared by processes through mapping;
  - Process can share memory for communication;
  - Pages can be shared during process creation with fork(), speedup

Demand Paging
1) Page is loaded when they are demanded during program execution;
2) Swapper (entire processors) V.S. Pager (individual pages of a process)
3) Page table for a entry is loaded as usual with unloaded page marked as invalid
4) Access invalid page cause page fault, causing trap to OS to bring designed
page back to memory
  - check an internal table (in PCB) to determine if access is valid;
  - invalid, terminate the process;
  - find a free frame, load memory and modify table entry flag
  - restart the instruction
5) locality of reference: multiple page faults per instruction unlikely to happen
6) The hardware support to demand paging: page table and secondary memory
7) instruction restart
  - difficulties for move from source to overlapped destination blocks
  - attempts to access both ends/use temporary registers
8) effective access time, page-fault rate
9) - Trap/save process state/determine interrupt/check valid/issue disk IO
   - CPU scheduling/Receive IO interrupt/Save the state/Service IO interrupt
   - Correct Page table/Wait for schedule/Restore and restart instruction

Copy-On-Write
1) Allow the parent and child processes initially to share the same pages, if
child attempts to modify a page, then create a copy and map it to the address
space of the child process.

Page Replacement
1) Over-allocation of memory happens when all memory is in use;
2) Solution is to use page replacement:
  - find a victim frame, write it to the disk, change its page and frame tables;
  - read the desire page and continue the process
  - requires two page transfer, can be relieved use modify bit or dirty bit;
3) frame-allocation algorithm/page-replacement algorithm
  - evaluated by running it on a reference string and compute #page faults
  - FIFO: Belady's anomaly (page-fault may increase as #allocated frames increases)
  - Optimal Page Replacement: Replace the page that will not be used for the
  longest period of time. (Requires future knowledge)
  - LRU: Replace the page that has not been used for the longest period of time.
  Implementation: time counters/stack using double-linked-list, both need hardware
  assistance
  - LRU-Approximation Page Replacement
  - Counting-Based Page Replacement
  - Page-Buffering Algorithms: systems commonly keep a pool of free frames, still
  choose a victim but allow process to restart ASAP;
4) Raw disk I/O bypasses all the filesystem services (demand paging/file locking
/prefetching/space allocation/file names and directories) to achieve efficiency;

Allocation of Frames
1) the minimum number of frames per process is defined by the architecture, the
maximum number is defined by the amount of available physical memory;
2) Allocation algorithms: Equal allocation/Proportional allocation (with priority)
3) Global replacement: select a replacement from the set of all frames;
  - depends on the paging behavior of other processes;
  - results in greater system throughput and more commonly used
Local replacement: select only its own set of allocated frames
4) non-uniform memory access (NUMA) track the last CPU on which each process run
and schedule/allocate frames close to the CPU to improve cache hits/decrease latency

Thrashing
1) A process is thrashing if it is spending more time paging than executing.
2) The OS increase the degree of multiprogramming by introducing a new process
to the system. increase paging -> decrease CPU utilization -> increase
multiprogramming and so on.
3) CPU utilization V.S. multiprogramming curve
4) locality model
  - set of pages that are actively used together
  - defined by program structure and its data structure
  - principal behind the caching
5) Working-set model is based on the assumption of locality. The set of pages in
the most recent delta page references is the working set;
6) if the total demand (summation of all process working set) is greater than the
available frames, thrashing will occur;
7) Page-fault frequency is monitored/controlled in range to prevent thrashing

Memory-Mapped files
1) Memory mapping a file allows a part of the virtual address space to be logically
associated with the file to increase performance
  - A page-sized portion of the file is read from the file system to physical page
  - Subsequent reads and writes to the file are handled as routine memory accesses
  - write to the file mapped in memory are not synchronous (periodically check
  and write before file close.)
2) Quite often, shared memory is in fact implemented by memory mapping files.
System provide special API to achieve that.
3) Many computer architectures provide memory-mapped I/O. Ranges of memory addresses
are set aside and are mapped to the device registers. Reads and writes to these
memory addresses cause the data to be transferred to and from the device registers.
for example, Displaying text on the screen is as easy as writing to memory...
4) Programmed I/O (polling) V.S. interrupt driven

Allocating Kernel Memory
1) Kernel must use memory conservatively and attempt to minimize waste due to
fragmentation
2) Certain hardware devices interact directly with physical memory require memory
residing in physically contiguous pages.

Other Considerations
1) Pre-paging is an attempt to prevent the high level of initial paging;
2) Page size consideration
  - size of the page table/minimize internal fragmentation
  - a desire to minimize I/O time argues for a larger page size.
  - smaller size result in better resolution, less IO and total allocated memory
  - minimize the number of page faults, we need a large page size
3) TLB reach refers to the amount of memory accessible from the TLB and is #entries
multiplied by the page size
4) System performance can be improved if the user (or compiler) has an awareness
of the underlying demand paging.
  - Careful selection of data structure can increase locality and hence lower the
  page-fault rate/#pages in the working set
  - stack has good locality/hash table scatter reference produce bad locality
  - the compiler and loader can have a significant effect on paging: 1) separating
  code (read only clean page) and data; 2) keep each routines completely in one
  page; 3) pack frequent routines into the same page, etc.
5) IO interlock: avoid I/O buffer being replaced/paged out
  - never execute IO to user memory, data copied between system and user memory
  - allow pages to be locked into memory, cannot be selected for replacement
6) Page Locking: 1) OS kernel locked in memory; 2) special user process (e.g.
database); 3) lock page for newly brought low-priority process pages.
